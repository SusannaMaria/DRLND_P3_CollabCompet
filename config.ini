[ddpg]

# replay buffer size of the memory containing the batch samples.
BUFFER_SIZE = 1000000
# minibatch size, number of samples used during an update (gradient descent).
BATCH_SIZE = 128
# Discount factor âˆˆ [0, 1] defines up to what extent future rewards influence the return in time step t.
GAMMA = 0.99        
# for soft update of target parameters
TAU = 0.001              

# learning rates defines the step size in solution direction and controls how strongly the weights of the artificial neural network are updated by the loss gradient.
# learning rate of the actor
LR_ACTOR = 0.001         
# learning rate of the critic
LR_CRITIC = 0.001
        
# L2 weight decay
WEIGHT_DECAY = 0     
# learning timestep interval   
LEARN_EVERY = 20     
# number of learning passes
LEARN_NUM = 10          
# Ornstein-Uhlenbeck noise parameter
OU_SIGMA = 0.2          
# Ornstein-Uhlenbeck noise parameter
OU_THETA = 0.15         
# explore->exploit noise process added to act step
EPSILON = 1.0           
# decay rate for noise process
EPSILON_DECAY = 0.000001    

EPSILON_FINAL = 0.0    

N_EPISODES = 150
MAX_T = 10000
SOLVED_SCORE = 30
SAVE_N_EPISODES = 5
CKPT_PATH = trained_test

[td3]
# replay buffer size
BUFFER_SIZE = 1000000
# minibatch size
BATCH_SIZE = 128
# discount factor
GAMMA = 0.99        
# for soft update of target parameters
TAU = 0.001              
# learning rate of the actor
LR_ACTOR = 0.001         
# learning rate of the critic
LR_CRITIC = 0.001        
# L2 weight decay
WEIGHT_DECAY = 0     
# learning timestep interval   
LEARN_EVERY = 1     
# number of learning passes
LEARN_NUM = 5          
# Ornstein-Uhlenbeck noise parameter
OU_SIGMA = 0.2          
# Ornstein-Uhlenbeck noise parameter
OU_THETA = 0.15         
# explore->exploit noise process added to act step
EPSILON = 1.0           
# decay rate for noise process
EPSILON_DECAY = 0.000001
EPSILON_FINAL = 0.0  

N_EPISODES = 150
MAX_T = 10000
SOLVED_SCORE = 30
SAVE_N_EPISODES = 100
CKPT_PATH = trained_test